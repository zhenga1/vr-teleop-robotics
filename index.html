<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="color-scheme" content="dark light">
  <title>Remote Teleoperation Data Collection | UR7e Ã— Meta Quest 3</title>
  <meta name="description" content="VR-based teleoperation for UR7e to collect real-world manipulation demonstrations, record trajectories, replay motion, and condition behavior using vision." />

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;600&display=swap" rel="stylesheet">

  <style>
    :root{
      /* Base colors */
      --bg: #f8fafc;
      --surface: rgba(255,255,255,0.75);
      --surface-strong: rgba(255,255,255,0.92);
      --text: #0f172a;
      --muted: #475569;
      --muted2: #64748b;

      /* Brand */
      --brand: #6366f1;
      --brand-soft: #eef2ff;
      --accent: #22d3ee;

      /* Semantic */
      --ok: #16a34a;
      --warn: #f59e0b;
      --bad: #dc2626;

      /* Gradients */
      --g-main: linear-gradient(135deg, #6366f1, #22d3ee);
      --g-soft: linear-gradient(135deg, #eef2ff, #f8fafc);

      /* Borders & shadows */
      --stroke: rgba(15,23,42,0.08);
      --shadow-sm: 0 8px 24px rgba(15,23,42,0.06);
      --shadow-md: 0 18px 50px rgba(15,23,42,0.10);
      --shadow-lg: 0 30px 80px rgba(15,23,42,0.14);

      /* Radius */
      --r-sm: 12px;
      --r: 16px;
      --r-lg: 22px;

      /* Layout */
      --max: 1120px;
    }

    *{ box-sizing: border-box; }
    html, body{ height: 100%; }
    body {
      margin: 0;
      font-family: Inter, system-ui, sans-serif;
      color: var(--text);
      background: #f8fafc;
    }



    a{ color: inherit; text-decoration: none; }
    a:hover{ text-decoration: underline; text-decoration-color: rgba(100,100,100,0.35); }
    .wrap{ width: min(var(--max), calc(100% - 40px)); margin: 0 auto; }

    /* Top nav */
    .nav{
      position: sticky;
      top: 0;
      z-index: 50;
      backdrop-filter: blur(16px);
      background: rgba(11,15,20,0.55);
      border-bottom: 1px solid rgba(255,255,255,0.08);
    }
    .nav-inner{
      display:flex; align-items:center; justify-content:space-between;
      padding: 14px 0;
    }
    .brand{
      display:flex; align-items:center; gap:12px;
      font-weight: 700; letter-spacing: -0.02em;
    }
    .mark{
      width: 34px; height: 34px; border-radius: 12px;
      background:
        radial-gradient(14px 14px at 30% 30%, rgba(106,228,255,0.9), transparent 60%),
        radial-gradient(18px 18px at 70% 60%, rgba(167,139,250,0.85), transparent 60%),
        linear-gradient(135deg, rgba(255,255,255,0.08), rgba(255,255,255,0.02));
      border: 1px solid rgba(255,255,255,0.18);
      box-shadow: 0 12px 40px rgba(0,0,0,0.35);
    }
    .nav-links{ display:flex; gap: 16px; align-items:center; }
    .nav-links a{
      font-size: 14px;
      padding: 8px 10px;
      color: white;
      border-radius: 12px;
      border: 1px solid transparent;
    }
    .nav-links a:hover{
      color: var(--text);
      border-color: rgba(255,255,255,0.10);
      background: rgba(255,255,255,0.04);
      text-decoration: none;
    }
    .cta{
      display:flex; gap:10px; align-items:center;
    }
    .btn {
      background: white;
      border: 1px solid var(--stroke);
      color: var(--text) !important;
    }

    .btn.primary{
      color: white;
      border: none;
      background: var(--g1);
      box-shadow: 0 20px 40px rgba(99,102,241,0.35);
    }

    .btn.primary:hover{
      transform: translateY(-1px);
      box-shadow: 0 28px 60px rgba(99,102,241,0.45);
    }
    .btn .dot{
      width: 8px; height: 8px; border-radius: 50%;
      background: var(--brand);
      box-shadow: 0 0 0 6px rgba(106,228,255,0.12);
    }

    /* Hero */
    header.hero{
      padding: 58px 0 26px;
    }
    .hero-grid{
      display:grid;
      grid-template-columns: 1.25fr 0.75fr;
      gap: 22px;
      align-items: stretch;
    }
    @media (max-width: 920px){
      .hero-grid{ grid-template-columns: 1fr; }
    }
    .card {
      background: linear-gradient(
        180deg,
        rgba(255,255,255,0.04),
        rgba(255,255,255,0.01)
      );
      border: 1px solid rgba(255,255,255,0.08);
      border-radius: 16px;
      padding: 24px;
      box-shadow: 0 20px 50px rgba(0,0,0,0.5);
    }

    .hero-card{
      padding: 28px 26px;
      position: relative;
      min-height: 340px;
    }
    .hero-card::before{
      content:"";
      position:absolute;
      inset:-3px;
      background:
        radial-gradient(600px 260px at 20% 20%, rgba(99,102,241,0.35), transparent 60%),
        radial-gradient(600px 260px at 80% 30%, rgba(34,211,238,0.30), transparent 60%),
        radial-gradient(500px 240px at 50% 90%, rgba(167,139,250,0.25), transparent 60%);
      z-index: 0;
      pointer-events:none;
    }

    .hero-card > *{ position: relative; z-index: 1; }
    .kicker{
      display:flex; gap:10px; flex-wrap:wrap;
      margin-bottom: 12px;
    }
    .pill{
      font-family: var(--mono);
      font-size: 12px;
      color: rgba(10,10,10,0.90);
      padding: 7px 10px;
      border-radius: 999px;
      border: 1px solid rgba(255,255,255,0.14);
      background: rgba(255,255,255,0.04);
    }
    .pill.ok{ border-color: rgba(52,211,153,0.28); background: rgba(52,211,153,0.08); }
    .pill.b{ border-color: rgba(106,228,255,0.25); background: rgba(106,228,255,0.07); }
    .pill.p{ border-color: rgba(167,139,250,0.25); background: rgba(167,139,250,0.07); }

    h1{
      margin: 10px 0 10px;
      font-size: clamp(30px, 4.2vw, 46px);
      line-height: 1.05;
      letter-spacing: -0.04em;
    }
    .sub{
      font-size: 16px;
      line-height: 1.55;
      color: var(--muted);
      max-width: 62ch;
      margin: 0 0 18px;
    }
    .hero-actions{
      display:flex; gap: 12px; flex-wrap:wrap;
      margin-top: 14px;
    }
    .meta{
      margin-top: 18px;
      display:flex; flex-wrap:wrap;
      gap: 10px 16px;
      color: var(--muted2);
      font-size: 13px;
    }
    .meta b{ color: rgba(49, 44, 44, 0.808); font-weight: 600; }

    /* Right rail */
    .side{
      padding: 18px;
      display:flex;
      flex-direction: column;
      gap: 12px;
      min-height: 340px;
    }
    .stat{
      border-radius: var(--r);
      border: 1px solid rgba(255,255,255,0.10);
      background: rgba(255,255,255,0.03);
      padding: 14px;
    }
    .stat .label{ font-size: 12px; color: var(--muted2); font-family: var(--mono); }
    .stat .value{ margin-top: 6px; font-size: 15px; color: var(--text); font-weight: 600; }
    .stat .hint{ margin-top: 6px; font-size: 13px; color: var(--muted); line-height: 1.45; }

    /* Sections */
    section{ padding: 26px 0; }
    .section-title{
      display:flex; align-items: baseline; justify-content: space-between;
      gap: 14px;
      margin-bottom: 14px;
    }
    h2{
      margin: 0;
      font-size: 22px;
      letter-spacing: -0.02em;
    }
    .section-title p{
      margin: 0;
      color: var(--muted2);
      font-size: 13px;
      font-family: var(--mono);
    }

    .grid-3{
      display:grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 14px;
    }
    @media (max-width: 980px){
      .grid-3{ grid-template-columns: 1fr; }
    }

    .panel{
      border-radius: var(--r2);
      border: 1px solid rgba(255,255,255,0.10);
      background: rgba(255,255,255,0.03);
      padding: 18px;
      box-shadow: 0 18px 60px rgba(0,0,0,0.30);
    }
    .panel h3{
      margin: 0 0 8px;
      font-size: 16px;
      letter-spacing: -0.02em;
    }
    .panel p{
      margin: 0;
      color: var(--muted);
      line-height: 1.55;
      font-size: 14px;
    }

    /* Pipeline / diagram-ish */
    .pipeline{
      display:grid;
      grid-template-columns: 1fr;
      gap: 14px;
      border: none !important;
    }
    .flow{
      display:grid;
      grid-template-columns: repeat(4, 1fr);
      gap: 12px;
      align-items: stretch;
    }
    @media (max-width: 980px){
      .flow{ grid-template-columns: 1fr; }
    }
    .node{
      border-radius: var(--r2);
      border: 1px solid rgba(0,0,0,0.8);
      background: linear-gradient(180deg, rgba(255,255,255,0.05), rgba(255,255,255,0.02));
      padding: 16px;
      position: relative;
      overflow:hidden;
    }
    .node::before{
      content:"";
      position:absolute; inset:-2px;
      background: radial-gradient(300px 120px at 30% 20%, rgba(106,228,255,0.10), transparent 60%);
      pointer-events:none;
    }
    .node:nth-child(2)::before{ background: radial-gradient(300px 120px at 30% 20%, rgba(167,139,250,0.10), transparent 60%); }
    .node:nth-child(3)::before{ background: radial-gradient(300px 120px at 30% 20%, rgba(52,211,153,0.08), transparent 60%); }
    .node:nth-child(4)::before{ background: radial-gradient(300px 120px at 30% 20%, rgba(251,191,36,0.08), transparent 60%); }
    .node > *{ position:relative; z-index:1; }
    .node .tag{
      font-family: var(--mono);
      font-size: 24px;
      color: rgba(10,0,255,0.75);
      opacity: 0.95;
    }
    .node .title{
      margin-top: 8px;
      font-weight: 700;
      letter-spacing: -0.02em;
    }
    .node ul{
      margin: 10px 0 0;
      padding-left: 18px;
      color: var(--muted);
      font-size: 13px;
      line-height: 1.55;
    }

    /* Demo gallery */
    .demo-grid{
      display:grid;
      grid-template-columns: 1.2fr 0.8fr;
      gap: 14px;
      align-items: stretch;
    }
    @media (max-width: 980px){
      .demo-grid{ grid-template-columns: 1fr; }
    }
    .demo-stage{
      border-radius: var(--r2);
      border: 1px solid rgba(255,255,255,0.10);
      background: rgba(255,255,255,0.6);
      padding: 22px;                        /* ðŸ‘ˆ more breathing room */
      box-shadow: 0 16px 40px rgba(2,6,23,0.12);
      display:flex;
      flex-direction: column;
      gap: 16px;
    }

    .media{
      border-radius: 18px;
      border: 1px dashed rgba(255,255,255,0.18);
      background:
        linear-gradient(135deg, rgba(255,255,255,0.06), rgba(255,255,255,0.02));
      min-height: 320px;
      display:grid;
      place-items: center;
      text-align:center;
      padding: 18px;
      position: relative;
      overflow:hidden;
    }
    .media::after{
      content:"";
      position:absolute; inset:-2px;
      background:
        radial-gradient(420px 220px at 25% 35%, rgba(106,228,255,0.10), transparent 60%),
        radial-gradient(420px 220px at 78% 60%, rgba(167,139,250,0.08), transparent 60%);
      pointer-events:none;
    }
    .media .ph{
      position:relative; z-index:1;
      max-width: 56ch;
    }
    .media .ph .t{
      font-weight: 700;
      letter-spacing: -0.02em;
      margin-bottom: 6px;
    }
    .media .ph .d{
      color: var(--muted);
      font-size: 14px;
      line-height: 1.55;
    }

    .demo-list{
      border-radius: var(--r2);
      border: 1px solid rgba(255,255,255,0.10);
      background: rgba(255,255,255,0.03);
      padding: 14px;
      box-shadow: 0 18px 60px rgba(0,0,0,0.28);
      display:flex;
      flex-direction: column;
      gap: 10px;
    }
    .demo-item{
      border-radius: 16px;
      border: 1px solid rgba(255,255,255,0.10);
      background: rgba(255,255,255,0.02);
      padding: 12px;
      cursor: pointer;
      transition: transform .15s ease, background .15s ease, border-color .15s ease;
    }
    .demo-item:hover{
      transform: translateY(-1px);
      border-color: rgba(106,228,255,0.18);
      background: rgba(255,255,255,0.04);
    }
    .demo-item .k{
      font-family: var(--mono);
      font-size: 12px;
      color: var(--muted2);
      margin-bottom: 6px;
    }
    .demo-item .n{
      font-weight: 700;
      letter-spacing: -0.01em;
      margin-bottom: 6px;
    }
    .demo-item .s{
      color: var(--muted);
      font-size: 13px;
      line-height: 1.45;
    }

    /* Footer */
    footer{
      padding: 26px 0 40px;
      color: var(--muted2);
      font-size: 13px;
    }
    .foot{
      display:flex; justify-content: space-between; gap: 12px; flex-wrap:wrap;
      border-top: 1px solid rgba(255,255,255,0.10);
      padding-top: 16px;
    }

    /* Modal */
    .modal{
      position: fixed; inset: 0;
      display:none;
      place-items: center;
      background: rgba(0,0,0,0.65);
      z-index: 100;
      padding: 22px;
    }
    .modal.open{ display:grid; }
    .modal-card{
      width: min(920px, 100%);
      border-radius: 22px;
      border: 1px solid rgba(255,255,255,0.14);
      background: rgba(14,18,24,0.92);
      box-shadow: 0 30px 120px rgba(0,0,0,0.70);
      overflow:hidden;
    }
    .modal-top{
      padding: 14px 16px;
      display:flex; justify-content: space-between; align-items:center;
      border-bottom: 1px solid rgba(255,255,255,0.10);
    }
    .modal-top .title{
      font-weight: 700;
      letter-spacing: -0.02em;
    }
    .modal-body{ padding: 16px; }
    .modal-body .frame{
      min-height: 420px;
      border-radius: 16px;
      border: 1px dashed rgba(255,255,255,0.18);
      background: rgba(255,255,255,0.03);
      display:grid;
      place-items:center;
      padding: 18px;
      text-align:center;
      color: var(--muted);
      line-height: 1.6;
    }
    .x{
      appearance:none;
      border: 1px solid rgba(255,255,255,0.14);
      background: rgba(255,255,255,0.05);
      color: var(--text);
      padding: 8px 10px;
      border-radius: 12px;
      cursor:pointer;
    }
    .x:hover{ background: rgba(255,255,255,0.07); }
    .vr-icon-img {
      width: 34px;
      height: 34px;
      padding: 6px;
      border-radius: 12px;
      background: rgba(255,255,255,0.06);
      border: 1px solid rgba(255,255,255,0.18);
      box-shadow: 0 10px 30px rgba(0,0,0,0.4);
    }
    .hidden {
      display: none;
    }

    #video-container {
      margin-top: 16px;
    }

    #demo-video {
      width: 420px;      /* ðŸ‘ˆ small */
      max-width: 100%;
      border-radius: 12px;
      box-shadow: 0 10px 30px rgba(0,0,0,0.25);
    }

    /* --- PASTE THIS AT THE END OF YOUR CSS --- */
    .video-container {
      position: relative;
      padding-bottom: 56.25%; /* 16:9 Aspect Ratio */
      height: 0;
      overflow: hidden;
      border-radius: 12px;
      background: #000;
      margin-bottom: 12px;
    }
    .video-container iframe {
      position: absolute;
      top: 0; left: 0;
      width: 100%; height: 100%;
      border: 0;
    }

  </style>
</head>

<body>
  <!-- NAV -->
  <div class="nav">
    <div class="wrap nav-inner">
      <div class="brand" aria-label="Project brand">
        <img src="assets/Quest-3-front-sensor-suite.png" class="vr-icon-img" alt="VR controller">
        <!--<img src="assets/quest3_right.png" class="vr-icon-img" alt="VR controller">-->
        <!-- <div class="mark" aria-hidden="true"></div> --->

        <div>
          <div style="color:white; font-size:14px; opacity:.95;">Remote Teleoperation</div>
          <div style="color: white; font-size:12px; margin-top:2px;">UR7e Ã— Meta Quest 3 â€¢ Data Collection</div>
        </div>
      </div>

      <nav class="nav-links" aria-label="Primary">
        <a href="#overview">Overview</a>
        <a href="#pipeline">Pipeline</a>
        <a href="#demos">Demos</a>
        <a href="#conclusion">Conclusion</a>
      </nav>

      <div class="cta">
        <a class="btn" href="#demos" title="Jump to demos">
          <span class="dot" aria-hidden="true"></span> View demos
        </a>
        <a class="btn primary" href="#contact" title="Contact / repo links">
          <span style="color: white; font-family:var(--mono); font-size:12px;">â†—</span> 
          <b style="color: white; font-family:var(--mono);">Get code</b>
        </a>
      </div>
    </div>
  </div>

  <!-- HERO -->
  <header class="hero">
    <div class="wrap hero-grid">
      <div class="card hero-card">
        <div class="kicker">
          <span class="pill b">VR teleop</span>
          <span class="pill p">trajectory logs</span>
          <span class="pill ok">replay on hardware</span>
          <span class="pill">vision-conditioned</span>
        </div>

        <h1>VR Teleoperation for Robot Data Collection & Demonstrations</h1>
        <p class="sub">
          We collect real-world robot demonstrations/trajectories on a UR7e using Meta Quest 3 controllers,
          store demonstrations in readable logs, and replay motion with conditional execution based on sensory input.
        </p>

        <div class="meta">
          <div><b>Platform:</b> UR7e</div>
          <div><b>Interface:</b> Meta Quest 3 (SteamVR/ALVR)</div>
          <div><b>Stack:</b> ROS 2 + MoveIt</div>
          <div><b>Outputs:</b> .txt logs (+ dataset placeholders)</div>
        </div>
      </div>

      <aside class="card side">
        <div class="stat">
          <div class="label">Project goal</div>
          <div class="value">Reliable remote data collection for robot learning</div>
          <div class="hint">Demonstrations/trajectories are recorded during teleoperation and replayed for training and evaluation.</div>
        </div>
        <div class="stat">
          <div class="label">Recording rate</div>
          <div class="value">2 Hz sampling (0.5 s)</div>
          <div class="hint">Aligned recording and replay for smoother execution.</div>
        </div>
        <div class="stat">
          <div class="label">Conditional execution</div>
          <div class="value">Color-based sorting â†’ replay</div>
          <div class="hint">Vision module selects which trajectory to execute based on color of objects detected.</div>
        </div>
      </aside>
    </div>
  </header>

  <!-- OVERVIEW -->
  <!-- <section id="overview">
    <div class="wrap">
      <div class="section-title">
        <h2>Overview</h2>
        <p>what & why</p>
      </div>

      <div class="grid-3">
        <div class="panel">
          <h3>Why data collection matters</h3>
          <p>
            Modern robot learning methods are data-hungryâ€”policy quality depends strongly on the diversity and fidelity
            of real-world demonstrations.
          </p>
        </div>
        <div class="panel">
          <h3>Our contribution</h3>
          <p>
            An intuitive VR teleoperation workflow using Meta Quest 3 controllers to collect UR7e manipulation trajectories,
            stored as readable logs for replay and dataset building.
          </p>
        </div>
        <div class="panel">
          <h3>Key capabilities</h3>
          <p>
            (1) VR-based 6-DoF control, (2) high-fidelity recording & storage, (3) trajectory replay on hardware,
            (4) vision-conditioned selection/sorting.
          </p>
        </div>
      </div>
    </div>
  </section> -->
  <section id="overview">
    <div class="wrap">
      <div class="section-title">
        <h2>Introduction</h2>
        <p>Goal, Motivation & Applications</p>
      </div>

      <div style="display: flex; flex-direction: column; gap: 24px;">

        <div class="panel">
          <h3 style="color: var(--brand);">Project Goal</h3>
          <p>
            Modern robot learning policies (including imitation learning and diffusion models) are constrained by a scarcity of diverse, high-quality training data, particularly for 7-DoF manipulation tasks. To address this, we developed a novel, low-cost teleoperation interface that combined Meta Quest 3 Virtual Reality headset with the UR7e robotic arm allowing a human operator to perform manipulation tasks naturally in 3D space through a headset while recording synchronized joint and gripper data to train modern Imitation Learning policies.
          </p>
        </div>

        <div class="panel">
          <h3 style="color: var(--accent);">Motivation & Technical Challenges</h3>
          <p>
            Modern robot learning (e.g., Diffusion Policies, Open X-Embodiment) is bottlenecked by the lack of high-quality demonstration data. Traditional collection methods such as kinesthetic teaching or "puppet" arms are either dangerous, unintuitive, expensive, or produce noisy and inconsistent results for complex tasks.
          </p>
          <p style="margin-top: 14px;">
            <strong>The Core Problem:</strong> This project bridges two distinct coordinate systems: the unconstrained Euclidean space of a VR controller and the kinematic constraints of an industrial robot. The key technical challenge was developing a <strong>"Logical Home Pairing" algorithm</strong> (Absolute Pose Mapping) to translate user hand poses into safe and accurate robot trajectories in real-time without risking singularities or sudden jumps.
          </p>
        </div>

        <div class="panel">
          <h3 style="color: var(--ok);">Real-World Applications</h3>
          <p>
            Beyond collecting training data for AI, this teleoperation architecture has immediate utility in several high-impact domains:
          </p>
          <ul style="margin: 12px 0 0; padding-left: 20px; color: var(--muted); font-size: 14px; line-height: 1.6;">
            <li style="margin-bottom: 6px;">
              <strong>Data Collection:</strong> Enabling scalable data generation for Open X-Embodiment initiatives.
            </li>
            <li style="margin-bottom: 6px;">
              <strong>Hazardous Environments:</strong> Allowing humans to handle toxic materials or nuclear waste from a safe distance using natural hand motions.
            </li>
            <li style="margin-bottom: 6px;">
              <strong>Remote Maintenance:</strong> Enabling technicians to repair machinery in cleanrooms or offshore rigs via VR digital twins.
            </li>
            <li>
              <strong>Space Robotics:</strong> Facilitating control of orbital manipulators where traditional joystick interfaces are too abstract for complex dexterity tasks.
            </li>
          </ul>
        </div>

      </div>
    </div>
  </section>

  <!-- PIPELINE  Maybe cahnge name to system-->
  <!-- <section id="pipeline">
    <div class="wrap">
      <div class="section-title">
        <h2>System Pipeline</h2>
        <p>inputs â†’ processing â†’ outputs</p>
      </div>

      <div class="pipeline">
        <div class="flow">
          <div class="node">
            <div class="tag">Planning / Control</div>
            <div class="title">VR Teleoperation</div>
            <ul>
              <li>Meta Quest 3 headset + controllers</li>
              <li>Coordinate mapping + pose transforms</li>
              <li>Inverse kinematics for UR7e end-effector targets</li>
            </ul>
          </div>
          <div class="node">
            <div class="tag">Data Collection</div>
            <div class="title">Record & Store</div>
            <ul>
              <li>Start/stop recording via ROS 2 services</li>
              <li>Sample joint state + gripper state</li>
              <li>Write readable <span style="font-family:var(--mono)">.txt</span> logs (dataset placeholders)</li>
            </ul>
          </div>
          <div class="node">
            <div class="tag">Actuation</div>
            <div class="title">Trajectory Replay</div>
            <ul>
              <li>Load recorded logs</li>
              <li>Timing alignment for smooth replay</li>
              <li>Execute on physical UR7e</li>
            </ul>
          </div>
          <div class="node">
            <div class="tag">Sensing</div>
            <div class="title">Vision-Based Selection</div>
            <ul>
              <li>Detect scene state (e.g., color labels)</li>
              <li>Populate job queue</li>
              <li>Replay selected trajectory conditionally</li>
            </ul>
          </div>
        </div>

        <div class="panel">
          <h3>Recorder lifecycle (editable)</h3>
          <p>
            <span style="font-family:var(--mono)">1)</span> launch recorder node â†’
            <span style="font-family:var(--mono)">2)</span> call <span style="font-family:var(--mono)">/start_recording</span> â†’
            <span style="font-family:var(--mono)">3)</span> teleoperate (timer sampling) â†’
            <span style="font-family:var(--mono)">4)</span> call <span style="font-family:var(--mono)">/stop_recording</span> to save joint + gripper states.
            <br><br>
            <span style="color:var(--muted2)">ros2 run planning teleo_recorder: Run teleo recorder</span>
            <br>
            <span style="color:var(--muted2)">ros2 run planning quest_teleop: Run teleop pipeline with Quest3</span>
            <br>
            <span style="color:var(--muted2)">ros2 run planning full_execute: Run replay with Quest3</span>
          </p>
        </div>
      </div>
    </div>
  </section> -->
  
  <section id="pipeline">
    <div class="wrap">
      
      <div class="section-title" style="margin-bottom: 40px;">
        <h2>Design Strategy</h2>
        <p>Criteria, Choices & Trade-offs</p>
      </div>

      <div class="grid-3" style="margin-bottom: 60px;">
        <div class="panel">
          <h3 style="color: var(--brand);">Design Criteria</h3>
          <p>
            To act as a viable data collection platform, the system had to meet three critical performance benchmarks:
          </p>
          <ul style="margin: 10px 0 0; padding-left: 20px; font-size: 13px; color: var(--muted); line-height: 1.6;">
            <li><strong>Latency:</strong> End-to-end delay must be <100ms to prevent user motion sickness and over-correction.</li>
            <li><strong>Safety:</strong> The system must prevent "jumps" when the VR controller is first activated.</li>
            <li><strong>Fidelity:</strong> Recorded trajectories must preserve high-frequency movements and gripper actions for successful replay.</li>
          </ul>
        </div>

        <div class="panel" style="grid-column: span 2;">
          <h3 style="color: var(--accent);">Design Choices & Trade-offs</h3>
          <p>
            We prioritized spatial intuition and operator ergonomics to ensure high-quality data collection. With this in mind we implemented an <strong>Absolute Pose Mapping</strong> strategy rather than Relative Velocity Control. 
          </p>
          <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 12px;">
            <div>
              <strong style="color: var(--text); font-size: 14px;">Why we chose it:</strong>
              <p style="font-size: 13px; margin-top: 4px;">
                Absolute mapping "locks" the robot's end-effector to the user's hand, enabling intuitive spatial understanding. Velocity control often suffers from drift which often makes precise stacking tasks frustrating for the user.
              </p>
            </div>
            <div>
              <strong style="color: var(--text); font-size: 14px;">The Trade-off:</strong>
              <p style="font-size: 13px; margin-top: 4px;">
                <strong>Workspace vs. Precision:</strong> By mapping 1:1, the user is physically limited by their own arm reach. We sacrificed the ability to move infinitely by resetting the controller and its pose in exchange for higher precision and safety in a fixed workspace.
              </p>
            </div>
          </div>
        </div>

        <div class="panel" style="grid-column: span 3;">
          <h3 style="color: var(--ok);">Engineering Impact</h3>
          <p>
            Our architecture moves beyond academic theory to solve the "Data Bottleneck" in industrial robotics. We prioritized portability and fault-tolerance to create a deployable solution.
          </p>
          <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 12px;">
            <div>
              <strong style="color: var(--text); font-size: 14px;">Scalable Data Pipeline (Quest 3):</strong>
              <p style="font-size: 13px; margin-top: 4px;">
                We replaced capital-intensive motion capture labs (e.g., OptiTrack) with consumer inside-out tracking. This drastically reduces the "cost per datum" and allows for decentralized data collection without specialized facility infrastructure.
              </p>
            </div>
            <div>
              <strong style="color: var(--text); font-size: 14px;">Solving the "Long Tail" (Human-in-the-Loop):</strong>
              <p style="font-size: 13px; margin-top: 4px;">
                Hard-coded automation fails on edge cases. Our teleoperation stack ensures reliability by providing a remote human fallback interface, allowing operators to resolve failures in hazardous or unstructured environments without stopping the line.
              </p>
            </div>
          </div>
        </div>
      </div>

      <!-- <div class="section-title"> -->





        
<div style="display: flex; flex-direction: column; width: 100%; max-width: 100%;">

  <section>
    <div class="section-title" style="margin-bottom: 30px;">
      <h2>Implementation</h2>
      <p>Full Pipeline</p>
    </div>

    <div style="margin-bottom: 40px;">
      <figure style="margin: 0; padding: 0; text-align: center;">
        <img 
          src="assets/pipeline_image.png" 
          alt="Full Pipeline" 
          style="width: 100%; max-width: 850px; height: auto; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); border: 1px solid rgba(0,0,0,0.05);"
        >
        <figcaption style="font-size: 13px; color: var(--muted); margin-top: 10px; text-align: center;">
          The complete pipeline showing real-time teleoperation (top) and the vision-based trajectory replay loop using Color Detection (bottom).
        </figcaption>
      </figure>
    </div>

    <div style="display: flex; align-items: center; justify-content: center; gap: 8px; flex-wrap: wrap;">
      <span class="pill p">Meta Quest 3</span>
      <span style="color: var(--muted); font-size: 18px;">â†’</span>
      
      <span class="pill">ALVR (Wireless)</span>
      <span style="color: var(--muted); font-size: 18px;">â†’</span>
      
      <span class="pill">SteamVR / OpenVR</span>
      <span style="color: var(--muted); font-size: 18px;">â†’</span>

      <span class="pill">Linux Workstation</span>
      <span style="color: var(--muted); font-size: 18px;">â†’</span>
      
      <span class="pill b">UR7e Manipulator</span>
    </div>
  </section>

  <section style="margin-top: 60px;">
    <h3 style="font-size: 18px; margin-bottom: 25px;">Software Architecture</h3>

    <div class="panel" style="margin-bottom: 30px;">
  <h3 style="color: var(--brand); border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 15px;">
    Meta Quest 3 TeleOp
  </h3>
  <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; align-items: start;">
    
    <div>
      <p style="font-size: 14px; color: var(--muted); margin-bottom: 15px;">
        The system implements a <strong>72Hz synchronous control loop</strong>. To ensure intuitive handling, we decouple the absolute coordinate systems using a "Logical Home" calibration. This allows the operator to map a comfortable hand position (P<sub>vr</sub>) to the robot's safety home (P<sub>robot</sub>) instantly.
      </p>
    </div>

    <div style="background: #f8f9fa; padding: 15px; border-radius: 6px; border-left: 3px solid var(--brand);">
      <!-- <strong style="font-size: 12px; color: #333; display: block; margin-bottom: 10px;">Control Law:</strong> -->
      
      <img src="./assets/IK quest pic.png" 
           alt="Equation showing T_target calculation based on calibration offset and live VR pose"
           style="max-width: 100%; height: auto; display: block;">
           
    </div>
  </div>
</div>

    <div class="panel" style="margin-bottom: 30px;">
      <h3 style="color: var(--accent); border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 15px;">
        Record Trajectory
      </h3>
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px;">
        <div>
          <p style="font-size: 14px; color: var(--muted); margin-bottom: 15px;">
            We provide users with the ability to record arbitrary teleoperation trajectories through a custom ROS 2 recording pipeline. Trajectory recording is initiated by launching the teleo_recorder ROS 2 node that we implemented, followed by invoking the service call
ros2 service call /start_recording std_srvs/srv/Trigger "{}".
Once triggered, the recording service begins capturing robot command messages published by the teleoperation stack (i.e., the command topic used to drive the robot during live control). These messages reflect the real-time motion generated during teleoperation and are logged continuously for the duration of the recording session. The resulting trajectories can then be replayed or processed downstream, enabling repeatable execution and offline analysis of teleoperated demonstrations.
          </p>
        </div>
        <div style="background: #f8f9fa; padding: 15px; border-radius: 6px; border-left: 3px solid var(--brand);">
          <!-- <strong style="font-size: 12px; color: #333; display: block; margin-bottom: 10px;">Control Law:</strong> -->
          
          <img src="./assets/vr_record_diagram.png" 
              alt="Equation showing T_target calculation based on calibration offset and live VR pose"
              style="max-width: 100%; height: auto; display: block;">
              
        </div>

      </div>
    </div>

    <div class="panel" style="margin-bottom: 30px;">
      <h3 style="color: var(--accent); border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 15px;">
        Replay Trajectory
      </h3>
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px;">
        <div>
          <p style="font-size: 14px; color: var(--muted); margin-bottom: 15px;">
            We provide the option for the user to replay any trajectory they recorded by passing in the path to the .txt file to a ROS2 node that populates a job queue with the joint positions recorded at each 0.5 second sample and executes trajectories to reach each of those joint positions one by one in a smooth fashion to replay the entire trajectory.
          </p>
      </div>
    </div>
    </div>

    <div class="panel" style="margin-bottom: 30px;">
      <h3 style="color: var(--ok); border-bottom: 1px solid #eee; padding-bottom: 10px; margin-bottom: 15px;">
        Continuous Trajectory Replay With Color Sensing
      </h3>
      <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px;">
        <div>
          <p style="font-size: 14px; color: var(--muted); margin-bottom: 15px;">
            Another option is for the user to run the trajectory replay continuously with color sensing. First, a ROS node must be run that loads the Intel Realsense D435i camera and publishes the detected color of objects to a ROS2 topic. For color detection, we filter colors using the HSV color spectrum, an adjustable distance (in our demo we used 0.75 meters to 1.1 meters), and adjustable area parameters to control the range of the size of objects that can be detected. Then a second ROS2 node must be run that pulls the information published by the camera, and once a color is detected, runs a full exection involving loading a trajectory .txt file that corresponds with the color of the object detected, populating a job queue with the joint positions recorded at each 0.5 second sample, and executing trajectories to reach each of those joint positions one by one in a smooth fashion to replay the entire trajectory. Once the trajectory has been replayed, the node returns to detecting colors and can replay another trajectory if another color is found.
          </p>
        </div>
      </div>
    </div>

      <div style="margin-top: 30px;">
        <figure style="margin: 0; padding: 0; text-align: center;">
          <img 
            src="assets/vision_image.jpeg" 
            alt="Vision Pipeline" 
            style="width: 100%; max-width: 850px; height: auto; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); border: 1px solid rgba(0,0,0,0.05);"
          >
          <figcaption style="font-size: 13px; color: var(--muted); margin-top: 10px; text-align: center;">
            The complete vision pipeline involves running a camera_detection ROS2 node that detects the color of a target object and publishes the color to the detected_color topic. A second ROS2 node full_execute must also be run that subscribes to the detected_color topic and based on the color detected populates a job queue to replay a recorded trajectory before listening back to the topic for the next detected color.
          </figcaption>
        </figure>
      </div>
    </div>

  <section>
  </div>
  <section id="demos" style="margin-bottom: 80px;">
    <div class="wrap">
      <div class="section-title">
        <h2>Results & Demos</h2>
      </div>

      <!-- <div class="demo-grid"> -->
        <!-- <div class="demo-stage">
          <div class="media hero-video">
            <div class="video-container">
              <iframe
                src="https://drive.google.com/file/d/1eMcpyJhRtLM1I6wXNmuv1Oq4-d7S8RSg/preview"
                width="420"
                height="236"
                allow="autoplay"
                style="border-radius:12px; border:none;">
              </iframe>
            </div>
            
            <div class="caption">
              End-to-end VR teleoperation pipeline that includes recording, vision, and replay on the UR7E. We perform the task of sorting blocks based on their color.
            </div>
          </div>
        </div> -->

        <!-- <div class="demo-stage">
          <div class="media small-video">
            <div class="video-container">
              <iframe
                src="https://drive.google.com/file/d/1a32AXZ5B5SqDEGQX2iF96X-ZKJQWQ4G7/preview"
                width="420"
                height="236"
                allow="autoplay"
                style="border-radius:12px; border:none;">
              </iframe>
            </div>
            <div class="caption">VR Teleoperation Demonstration</div>
          </div>
        </div>

          <div class="demo-stage">
          <div class="media small-video">
            <div class="video-container">
              <iframe
                src="https://drive.google.com/file/d/1lp2n5Cn-3_QCeh4_zbTvo7GFP2QbZuLO/preview"
                width="420"
                height="236"
                allow="autoplay"
                style="border-radius:12px; border:none;">
              </iframe>
            </div>
            <div class="caption">Trajectory Replay Demonstration With Sorting</div>
          </div>
          </div> -->

        <div class="grid-3" style="grid-template-columns: 1fr 1fr;">
        
        <div style="display: flex; flex-direction: column;">
          <div class="video-container">
            <iframe src="https://drive.google.com/file/d/1a32AXZ5B5SqDEGQX2iF96X-ZKJQWQ4G7/preview" allow="autoplay"></iframe>
          </div>
          <div style="text-align: center; font-size: 14px; color: var(--muted); margin-top: 8px;">
            <strong>VR Teleoperation</strong>
          </div>
        </div>

        <div style="display: flex; flex-direction: column;">
          <div class="video-container">
            <iframe src="https://drive.google.com/file/d/1lp2n5Cn-3_QCeh4_zbTvo7GFP2QbZuLO/preview" allow="autoplay"></iframe>
          </div>
          <div style="text-align: center; font-size: 14px; color: var(--muted); margin-top: 8px;">
             <strong>Autonomous Replay</strong>
          </div>
        </div>
      </div>

        </div>
  </section>

  <section id="conclusion">
    <div class="wrap">
      
      <div class="section-title" style="margin-bottom: 40px;">
        <h2>Conclusion</h2>
      </div>

      <div class="grid-3" style="margin-bottom: 60px;">
        
        <div class="panel" style="grid-column: span 2;">
          <h3 style="color: var(--brand);">Results</h3>
          <p>
            Overall, our system successfully met our core design criteria. We demonstrated a complete end-to-end VR-based teleoperation pipeline from the Meta Quest 3 to the UR7e robot arm, including real-time control, trajectory recording, and reliable trajectory replay. This outcome closely aligns with our initial project goals and the scope refined after meeting with our TA.
          </p>

          <p>
            In particular, the system achieved stable and intuitive teleoperation, allowing a human operator wearing the Quest 3 headset to control the UR7eâ€™s end-effector motion in real time with low perceptual latency. The mapping between VR controller motion and robot motion was sufficiently smooth to enable precise manipulation behaviors, validating our design choice of using VR as a natural and expressive humanâ€“robot interface. Users were able to guide the robot through meaningful manipulation trajectories with minimal training.
          </p>

          <p>
            Trajectory recording and replay functioned as intended and served as a strong indicator of system robustness. Recorded demonstrations could be replayed consistently, with the robot closely following the original motion paths. This confirms that our trajectory encoding, time synchronization, and playback solutions preserved the essential structure of the human demonstrations. These results make possible our broader motivation of using teleoperation as a high-quality data collection method.
          </p>
            
          </p>
        </div>

        <div class="panel">
          <h3 style="color: var(--accent);">Challenges/Difficulties</h3>
          <ul style="margin: 10px 0 0; padding-left: 20px; font-size: 13px; color: var(--muted); line-height: 1.6;">
              <li><strong>Latency:</strong> Hardware: Getting the Quest headset + controllers reliably connected on the same local network as the lab computer.</li>
              <li><strong>Safety:</strong> Software: Ensuring a smooth trajectory replay by through consistent timing between recorded samples and playback speed.</li>
              <li><strong>Fidelity:</strong> Sensing: Ensuring trajectories can run continously based on sensory input in different environments (lighting, distance, shadows, etc.).</li>
          </ul>
        </div>

        <div class="panel" style="grid-column: span 3;">
          <h3 style="color: var(--ok);">Flaws/Future Considerations</h3>
          <p>
            One flaw in our system is that it doesn't have a safety net to prevent collisions if the human operator makes a mistake while controlling it. For example, if the human operator drops a controller the arm will go down and may crash into the table. To prevent this issue from hapening, we aim to add robot workspace bounding and proximity checks to prevent unsafe motions. Furthermore, instead of a basic color sensing pipeline, we aim to incorporate reinforcement learning to build upon the teleoperated demonstrations collected in this project. Rather than relying on task-specific object placement, learned policies will adapt the demonstrated trajectories to new object poses and environmental variations, improving robustness and generalization.
          </p>
        </div>

      </div>
    </div>
  </section>

  <section id="team" style="margin-top: 80px; margin-bottom: 80px;">
    
    <div style="max-width: 1100px; margin: 0 auto; padding: 0 20px;">

      <div class="section-title" style="border-top: 1px solid #eee; padding-top: 40px; margin-bottom: 30px;">
        <h2>Project Team</h2>
        <p>Roles & Contributions</p>
      </div>

      <div style="display: flex; flex-direction: column; gap: 20px;">

        <div class="panel" style="display: flex; flex-wrap: wrap; gap: 30px; align-items: start;">
          <div style="flex: 0 0 220px; min-width: 200px;">
            <h3 style="color: var(--brand); margin-bottom: 5px; font-size: 18px;">Samuel Mankoff</h3>
            <span style="font-size: 11px; text-transform: uppercase; letter-spacing: 0.5px; color: #999; display: block; line-height: 1.4;">
              Masterâ€™s Student<br>Mechanical Engineering
            </span>
          </div>
          <div style="flex: 1; min-width: 280px;">
            <p style="font-size: 14px; color: var(--muted); margin-bottom: 12px;">
              Interested in robotic control, perception, and learning. Focused on designing robots that can replicate human manipulation skills.
            </p>
            <div style="padding-top: 10px; border-top: 1px solid #f0f0f0;">
              <strong style="font-size: 12px; color: #333;">Key Contribution:</strong>
              <span style="font-size: 13px; color: var(--muted);">
                Led development of the VR-based teleoperation pipeline (Quest 3). Assisted with robot control logic and trajectory mapping.
              </span>
            </div>
          </div>
        </div>

        <div class="panel" style="display: flex; flex-wrap: wrap; gap: 30px; align-items: start;">
          <div style="flex: 0 0 220px; min-width: 200px;">
            <h3 style="color: var(--brand); margin-bottom: 5px; font-size: 18px;">Akshaj Gupta</h3>
            <span style="font-size: 11px; text-transform: uppercase; letter-spacing: 0.5px; color: #999; display: block; line-height: 1.4;">
              Sophomore<br>EECS
            </span>
          </div>
          <div style="flex: 1; min-width: 280px;">
            <p style="font-size: 14px; color: var(--muted); margin-bottom: 12px;">
              Focused on control and learning algorithms for robotic arms and humanoids to perform difficult manipulation tasks.
            </p>
            <div style="padding-top: 10px; border-top: 1px solid #f0f0f0;">
              <strong style="font-size: 12px; color: #333;">Key Contribution:</strong>
              <span style="font-size: 13px; color: var(--muted);">
                Developed the Computer Vision Pipeline (HSV/Depth) and implemented the trajectory saving/replay logic.
              </span>
            </div>
          </div>
        </div>

        <div class="panel" style="display: flex; flex-wrap: wrap; gap: 30px; align-items: start;">
          <div style="flex: 0 0 220px; min-width: 200px;">
            <h3 style="color: var(--brand); margin-bottom: 5px; font-size: 18px;">Kourosh Salahi</h3>
            <span style="font-size: 11px; text-transform: uppercase; letter-spacing: 0.5px; color: #999; display: block; line-height: 1.4;">
              Junior<br>EECS & Business
            </span>
          </div>
          <div style="flex: 1; min-width: 280px;">
            <p style="font-size: 14px; color: var(--muted); margin-bottom: 12px;">
              Focused on machine learning applications for robotics, specifically control strategies for humanoid platforms.
            </p>
            <div style="padding-top: 10px; border-top: 1px solid #f0f0f0;">
              <strong style="font-size: 12px; color: #333;">Key Contribution:</strong>
              <span style="font-size: 13px; color: var(--muted);">
                Contributed to the CV sensing pipeline for object detection and performed data collection trials via teleoperation.
              </span>
            </div>
          </div>
        </div>

        <div class="panel" style="display: flex; flex-wrap: wrap; gap: 30px; align-items: start;">
          <div style="flex: 0 0 220px; min-width: 200px;">
            <h3 style="color: var(--brand); margin-bottom: 5px; font-size: 18px;">Ziteng (Ender) Ji</h3>
            <span style="font-size: 11px; text-transform: uppercase; letter-spacing: 0.5px; color: #999; display: block; line-height: 1.4;">
              Senior<br>Computer Science
            </span>
          </div>
          <div style="flex: 1; min-width: 280px;">
            <p style="font-size: 14px; color: var(--muted); margin-bottom: 12px;">
              Research interests lie in robot learning, specifically enabling robots to acquire sophisticated skills through data-driven approaches.
            </p>
            <div style="padding-top: 10px; border-top: 1px solid #f0f0f0;">
              <strong style="font-size: 12px; color: #333;">Key Contribution:</strong>
              <span style="font-size: 13px; color: var(--muted);">
                Architected the Meta Quest connection with UR7e and assisted in the physical setup of the teleoperation workspace.
              </span>
            </div>
          </div>
        </div>

        <div class="panel" style="display: flex; flex-wrap: wrap; gap: 30px; align-items: start;">
          <div style="flex: 0 0 220px; min-width: 200px;">
            <h3 style="color: var(--brand); margin-bottom: 5px; font-size: 18px;">Aaron Zheng</h3>
            <span style="font-size: 11px; text-transform: uppercase; letter-spacing: 0.5px; color: #999; display: block; line-height: 1.4;">
              Senior<br>EECS
            </span>
          </div>
          <div style="flex: 1; min-width: 280px;">
            <p style="font-size: 14px; color: var(--muted); margin-bottom: 12px;">
              Interested in humanoid robots and LLMs. Focusing on learning-based policies for motion execution in simulation.
            </p>
            <div style="padding-top: 10px; border-top: 1px solid #f0f0f0;">
              <strong style="font-size: 12px; color: #333;">Key Contribution:</strong>
              <span style="font-size: 13px; color: var(--muted);">
                Integrated Meta Quest 3 (ROS2 -> SteamVR â†’ ALVR) for UR7e teleoperation. Implemented robust color detection for obj detection, contributed to teleo / record script.
              </span>
            </div>
          </div>
        </div>

      </div>
    </div>
  </section>

  <section id="materials">
    <div style="border-top: 1px solid #e0e0e0; margin-bottom: 60px;"></div>

    <div class="wrap" style="margin-bottom: 60px;">
      
      <div class="section-title" style="margin-bottom: 40px;">
        <h2>Additional Materials</h2>
        <p>Documentation & Resources</p>
      </div>

      <div style="display: flex; flex-direction: column; gap: 30px;">
        
        <div class="panel">
          <h3 style="color: var(--brand);">Project Presentation</h3>
          <p style="margin-bottom: 20px;">
            Our comprehensive final presentation covering the complete development process, technical challenges, experimental results, and future work:
          </p>
          
          <div style="border-radius: 16px; border: 1px solid rgba(255,255,255,0.10); background: rgba(255,255,255,0.03); padding: 20px; box-shadow: 0 18px 60px rgba(0,0,0,0.28);">
            <div style="margin-bottom: 16px;">
              <strong style="color: var(--text); font-size: 16px;">ME 206A Final Presentation - VR Teleoperation for Robot Learning</strong>
              <p style="margin: 8px 0; color: var(--muted); font-size: 14px;">
                Complete technical overview including motivation, design decisions, implementation details, experimental results, and team contributions.
              </p>
            </div>
            
            <div style="position: relative; width: 100%; height: 600px; border-radius: 12px; overflow: hidden; border: 1px solid rgba(255,255,255,0.08);">
              <iframe 
                src="./106_206A final presentation.pdf" 
                width="100%" 
                height="100%" 
                style="border: none; background: white;">
                <p style="color: var(--muted); text-align: center; padding: 40px;">
                  Your browser does not support PDF viewing. 
                  <a href="./106_206A final presentation.pdf" target="_blank" style="color: var(--brand); text-decoration: underline;">
                    Click here to download the presentation
                  </a>
                </p>
              </iframe>
            </div>
            
            <div style="margin-top: 12px; display: flex; gap: 12px; align-items: center;">
              <a href="./106_206A final presentation.pdf" 
                 target="_blank" 
                 style="display: inline-flex; align-items: center; gap: 8px; padding: 8px 12px; background: var(--brand); color: white; border-radius: 8px; text-decoration: none; font-size: 14px; font-weight: 600;">
                <span>ðŸ“„</span> Download PDF
              </a>
              <span style="color: var(--muted); font-size: 13px;">
                Complete slide deck with technical details and experimental results
              </span>
            </div>
          </div>
        </div>

      </div>
    </div>
  </section>

  <section id="contact">
    <div class="wrap">
      <div class="section-title">
        <h2>Links</h2>
      </div>

      <div class="grid-3">
        <div class="panel">
          <h3>Team</h3>
          <p>
            B.S.: Aaron Zheng, Akshaj Gupta, Kourosh Salahi, Ziteng (Ender) Ji<br>
            M.S.: Samuel Mankoff
          </p>
        </div>
        <div class="panel">
          <h3>Repository</h3>
          <p>
            <span style="color:var(--muted2)">Link:</span><br>
            <span style="font-family:var(--mono); font-size:12px;">
              <a href="https://github.com/samuel-mankoff/meceng206a-Final-Project" target="_blank" style="color:inherit; text-decoration:underline;">
                https://github.com/samuel-mankoff/meceng206a-Final-Project
              </a>
            </span>
          </p>
        </div>
        <div class="panel">
          <h3>Acknowledgements</h3>
          <p>
            We would like to thank EECS 106A Course Staff for helping us throughout this project
          </p>
        </div>
      </div>
    </div>
  </section>
 <footer>
  <div class="wrap foot">
    <div> <span id="year" style="font-family:var(--mono); font-size: 16px;"></span> Remote Teleoperation Data Collection â€¢ ME206A Project Group 18</div>
    <div style="display:flex; gap:14px; flex-wrap:wrap;">
      <a href="#overview">Overview</a>
      <a href="#pipeline">Pipeline</a>
      <a href="#demos">Demos</a>
      <a href="#contact">Contact</a>
    </div>
  </div>
 </footer>
</body></html>
